+ source activate vilbert
++ _CONDA_ROOT=/nethome/halamri3/anaconda_3
++ . /nethome/halamri3/anaconda_3/etc/profile.d/conda.sh
+++ export CONDA_EXE=/nethome/halamri3/anaconda_3/bin/conda
+++ CONDA_EXE=/nethome/halamri3/anaconda_3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/nethome/halamri3/anaconda_3/bin/python
+++ CONDA_PYTHON_EXE=/nethome/halamri3/anaconda_3/bin/python
+++ '[' -z x ']'
++ conda activate vilbert
++ '[' 2 -lt 1 ']'
++ local cmd=activate
++ shift
++ case "$cmd" in
++ __conda_activate activate vilbert
++ '[' -n '' ']'
++ local cmd=activate
++ shift
++ local ask_conda
++ OLDPATH=/nethome/halamri3/anaconda_3/bin:/nethome/halamri3/anaconda_3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
++ __add_sys_prefix_to_path
++ '[' -n '' ']'
+++ dirname /nethome/halamri3/anaconda_3/bin/conda
++ SYSP=/nethome/halamri3/anaconda_3/bin
+++ dirname /nethome/halamri3/anaconda_3/bin
++ SYSP=/nethome/halamri3/anaconda_3
++ '[' -n '' ']'
++ PATH=/nethome/halamri3/anaconda_3/bin:/nethome/halamri3/anaconda_3/bin:/nethome/halamri3/anaconda_3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
++ export PATH
+++ PS1=
+++ /nethome/halamri3/anaconda_3/bin/conda shell.posix activate vilbert
++ ask_conda='PS1='\''(vilbert) '\''
export PATH='\''/nethome/halamri3/anaconda_3/envs/vilbert/bin:/nethome/halamri3/anaconda_3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games'\''
export CONDA_PREFIX='\''/nethome/halamri3/anaconda_3/envs/vilbert'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''vilbert'\''
export CONDA_PROMPT_MODIFIER='\''(vilbert) '\''
export CONDA_EXE='\''/nethome/halamri3/anaconda_3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/nethome/halamri3/anaconda_3/bin/python'\''
export CONDA_PREFIX_1='\''/nethome/halamri3/anaconda_3'\'''
++ PATH=/nethome/halamri3/anaconda_3/bin:/nethome/halamri3/anaconda_3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
++ eval 'PS1='\''(vilbert) '\''
export PATH='\''/nethome/halamri3/anaconda_3/envs/vilbert/bin:/nethome/halamri3/anaconda_3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games'\''
export CONDA_PREFIX='\''/nethome/halamri3/anaconda_3/envs/vilbert'\''
export CONDA_SHLVL='\''2'\''
export CONDA_DEFAULT_ENV='\''vilbert'\''
export CONDA_PROMPT_MODIFIER='\''(vilbert) '\''
export CONDA_EXE='\''/nethome/halamri3/anaconda_3/bin/conda'\''
export _CE_M='\'''\''
export _CE_CONDA='\'''\''
export CONDA_PYTHON_EXE='\''/nethome/halamri3/anaconda_3/bin/python'\''
export CONDA_PREFIX_1='\''/nethome/halamri3/anaconda_3'\'''
+++ PS1='(vilbert) '
+++ export PATH=/nethome/halamri3/anaconda_3/envs/vilbert/bin:/nethome/halamri3/anaconda_3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
+++ PATH=/nethome/halamri3/anaconda_3/envs/vilbert/bin:/nethome/halamri3/anaconda_3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games
+++ export CONDA_PREFIX=/nethome/halamri3/anaconda_3/envs/vilbert
+++ CONDA_PREFIX=/nethome/halamri3/anaconda_3/envs/vilbert
+++ export CONDA_SHLVL=2
+++ CONDA_SHLVL=2
+++ export CONDA_DEFAULT_ENV=vilbert
+++ CONDA_DEFAULT_ENV=vilbert
+++ export 'CONDA_PROMPT_MODIFIER=(vilbert) '
+++ CONDA_PROMPT_MODIFIER='(vilbert) '
+++ export CONDA_EXE=/nethome/halamri3/anaconda_3/bin/conda
+++ CONDA_EXE=/nethome/halamri3/anaconda_3/bin/conda
+++ export _CE_M=
+++ _CE_M=
+++ export _CE_CONDA=
+++ _CE_CONDA=
+++ export CONDA_PYTHON_EXE=/nethome/halamri3/anaconda_3/bin/python
+++ CONDA_PYTHON_EXE=/nethome/halamri3/anaconda_3/bin/python
+++ export CONDA_PREFIX_1=/nethome/halamri3/anaconda_3
+++ CONDA_PREFIX_1=/nethome/halamri3/anaconda_3
++ __conda_hashr
++ '[' -n '' ']'
++ '[' -n '' ']'
++ hash -r
+ python train_language_only_baseline.py -enable_visdom 1 -visdom_server http://asimo.cc.gatech.edu -visdom_server_port 7776 -batch_size 60 -lr 4e-05 -image_lr 2e-05 -batch_multiply 1 -num_options 100 -lm_loss_coeff 1 -num_epochs 1 -mask_prob 0.1 -n_gpus 8 -sequences_per_image 8 -nsp_loss_coeff 1 -num_negative_samples 1 -visdial_tot_rounds 11 -visdom_env debugvisdial_batch_60_lr_4e-05_ilr_2e-05_bm_1_lmcoeff_1_nspcoeff_1_maskprob_0.1_spi_8_tot_rnds_11_job -save_name 06-Jan-20-18:35:47-MonDubgvisdial_batch_60_lr_4e-05_ilr_2e-05_bm_1_lmcoeff_1_nspcoeff_1_maskprob_0.1_spi_8_tot_rnds_11_job
01/06/2020 15:50:51 - WARNING - visdom -   Setting up a new session...
01/06/2020 15:50:51 - INFO - visdom -   Visdom successfully connected to server
01/06/2020 15:50:51 - INFO - __main__ -   {'visdial_processed_train': 'data/data/avsd/train_options.json', 'visdial_processed_val': 'data/data/avsd/val_options.json', 'visdial_processed_test': 'data/data/visdial/visdial_1.0_test_processed.json', 'visdial_image_feats': 'data/data/visdial/visdial_img_feat.lmdb', 'visdial_processed_train_dense': 'data/data/visdial/visdial_1.0_train_dense_processed.json', 'visdial_processed_train_dense_annotations': 'data/visdial/visdial_1.0_train_dense_annotations_processed.json', 'visdial_processed_val_dense_annotations': 'data/data/visdial/visdial_1.0_val_dense_annotations_processed.json', 'start_path': '', 'model_config': 'config/bert_base_baseline.json', 'enable_visdom': 1, 'visdom_env': 'debugvisdial_batch_60_lr_4e-05_ilr_2e-05_bm_1_lmcoeff_1_nspcoeff_1_maskprob_0.1_spi_8_tot_rnds_11_job', 'visdom_server': 'http://asimo.cc.gatech.edu', 'visdom_server_port': 7776, 'num_workers': 8, 'batch_size': 60, 'num_epochs': 1, 'batch_multiply': 1, 'lr': 4e-05, 'image_lr': 2e-05, 'overfit': False, 'continue': False, 'num_train_samples': 0, 'num_val_samples': 0, 'num_options': 100, 'n_gpus': 8, 'sequences_per_image': 8, 'visdial_tot_rounds': 11, 'max_seq_len': 256, 'num_negative_samples': 1, 'lm_loss_coeff': 1.0, 'nsp_loss_coeff': 1.0, 'img_loss_coeff': 1, 'mask_prob': 0.1, 'save_path': 'checkpoints/06-Jan-20-18:35:47-MonDubgvisdial_batch_60_lr_4e-05_ilr_2e-05_bm_1_lmcoeff_1_nspcoeff_1_maskprob_0.1_spi_8_tot_rnds_11_job', 'save_name': '06-Jan-20-18:35:47-MonDubgvisdial_batch_60_lr_4e-05_ilr_2e-05_bm_1_lmcoeff_1_nspcoeff_1_maskprob_0.1_spi_8_tot_rnds_11_job'}
01/06/2020 15:50:54 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /nethome/halamri3/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084
01/06/2020 15:50:58 - INFO - pytorch_transformers.modeling_utils -   loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /nethome/halamri3/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c
01/06/2020 15:50:58 - INFO - pytorch_transformers.modeling_utils -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "finetuning_task": null,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "num_labels": 2,
  "output_attentions": false,
  "output_hidden_states": true,
  "pruned_heads": {},
  "torchscript": false,
  "type_vocab_size": 2,
  "vocab_size": 30522
}

01/06/2020 15:50:58 - INFO - pytorch_transformers.modeling_utils -   loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /nethome/halamri3/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157
01/06/2020 15:51:05 - INFO - pytorch_transformers.modeling_utils -   Weights of BertForPretrainingDialog not initialized from pretrained model: ['bert.embeddings.token_type_embeddings_extension.weight', 'bert.embeddings.sep_embeddings.weight']
01/06/2020 15:51:05 - INFO - __main__ -   1140 iter per epoch.
/nethome/halamri3/visdial-bert/utils/data_utils.py:10: UserWarning: torch.range is deprecated in favor of torch.arange and will be removed in 0.5. Note that arange generates values in [start; end), not [start; end].
  seq_range = torch.range(0, max_len - 1).long()
/nethome/halamri3/anaconda_3/envs/vilbert/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
/nethome/halamri3/anaconda_3/envs/vilbert/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
01/06/2020 15:51:34 - INFO - __main__ -   [%s][Ep: %.2f][Iter: %d][Time: %5.2fs][NSP + LM Loss: %.3g][LM Loss: %.3g][NSP Loss: %.3g]
01/06/2020 15:51:34 - INFO - __main__ -   Iter: [0], Tot_loss: [7.118495]
01/06/2020 15:51:34 - INFO - __main__ -   Lm + nsp Loss: 7.118495
01/06/2020 15:51:34 - INFO - __main__ -   Lm + nsp Loss: 2.173724
01/06/2020 15:51:34 - INFO - __main__ -   Lm + nsp Loss: 4.944772
01/06/2020 15:51:34 - INFO - __main__ -   num iteration for eval
